{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to web scraping with Python\n",
    "\n",
    "Today, we'll be learning about using Python to collect data publicly available on the Internet through what is commonly referred to as web scraping. \n",
    "\n",
    "#### Website \n",
    "\n",
    "We'll be scraping a [listing of public financial disclosures](https://extapps2.oge.gov/201/Presiden.nsf/PAS%20Filings%20by%20Date?OpenView) filed by top federal government employees posted by the Office of Government Ethics. This listing is not provided in a downloadable format but instead as an HTML table that we'll be using some common Python packages to extract. \n",
    "\n",
    "Before we get started, we'll want to use the inspector in our browser to examine the pattern of how the data is stored and displayed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import statements \n",
    "\n",
    "To use a Python library in a script, we have to import it. You do that using what are known as import statements. Below we'll import the packages that we installed and that we'll be using to scrape the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign the url to a variable\n",
    "\n",
    "In Python, variables are assigned using the `=` with the variable name to the left and the contents of the variable to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://extapps2.oge.gov/201/Presiden.nsf/PAS%20Filings%20by%20Date?OpenView'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up our header (Scraping ethically and openly)\n",
    "\n",
    "When scraping, it is best practice to be transparent about what you're doing and kind to the websites you're visiting. Below we'll construct a header that we'll send along with our http request when we visit the website so that the webmaster of the site would be able to see what we're doing in their logs. In most cases, you just need to include your email address so they know how to contact you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36 (chad.day@wsj.com)'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visit our website (Send an HTTP request with `requests`)\n",
    "\n",
    "Here we'll use the Python package `requests`, which does basically what your browser does when it visits a website. What we get back is generally referred to as the response. A part of that response is the page HTML. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url, headers=header)\n",
    "\n",
    "html = r.text\n",
    "\n",
    "#pprint.pprint(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look closely at one entry\n",
    "\n",
    "It's easier to do this using your browser inspector but I've included one of our rows of data below. What patterns do you see? "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<tr valign=\"top\">\n",
    "    <td nowrap>\n",
    "        <font size=\"2\" face=\"Verdana\">12/07/2021</font></td>\n",
    "    <td nowrap><font size=\"2\" ''face=\"Verdana\">\n",
    "        <a href='https://extapps2.oge.gov/201/Presiden.nsf/PAS+Index/CA12044E3E1DB208852587A40033D508/$FILE/Young, \"Shalanda  final278.pdf'>Nominee 278 (12/03/2021)</a>\n",
    "        </font></td>\n",
    "    <td 'nowrap><font size=\"2\" face=\"Verdana\">Young, Shalanda</font></td>\n",
    "        <td 'nowrap><font size=\"2\" face=\"Verdana\">Office of Management and Budget</font</td>\n",
    "        <td nowrap><font size=\"2\" 'face=\"Verdana\">Director</font></td>\n",
    "</tr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing HTML \n",
    "\n",
    "To collect this data, we'll need to use a process called `parsing` and use a package called `BeautifulSoup`, which allows us to navigate the structure of HTML and leverage the patterns we see to select the pieces and parts we want. \n",
    "\n",
    "To do this, we feed in the html text and have BeautifulSoup parse it. We then assign that parsed html the variable name `soup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, looking closely at our table on the website, we'll want to look for a couple things. \n",
    "\n",
    "1. The HTML heirarchy that contains our whole table.\n",
    "2. The HTML tag for a row\n",
    "3. The HTML tag for a cell of a row\n",
    "\n",
    "\n",
    "#### Find the table\n",
    "\n",
    "Since we're dealing with an HTML table, we'll see that the whole table of data is contained between tags simple called `<table>` and `</ table>`. In BeautifulSoup, we can search across the parsed HTML and look for all instances of a tag using `find_all`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = soup.find_all('table')\n",
    "\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cutting into `iterable` chunks\n",
    "Now within the table, we'll want to split out each row so we can do what's called `iterating` through the data. One of the most common ways of iterating in Python is to use what's called a `for loop`. This translates basically to do something for each item in a group. In Python, we'll be creating a list containing each row so we can iterate through each one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = table[0].find_all('tr')\n",
    "\n",
    "rows[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of our rows, let's focus on gathering five pieces of information from each row. \n",
    "\n",
    " - Date\n",
    " - Link to filings\n",
    " - Name of filing\n",
    " - Filer's name\n",
    " - Filer's title\n",
    "\n",
    "These pieces of data are contained in the `<td>` tags, so we'll use them when parsing each row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Python containers for our data\n",
    "\n",
    "We've located our data. Now we need to set up a mechanism for storing it in Python so we can work with it. We'll use two types of Python data structures for this. One is a list, which we used before. The other is a `dictionary`, which uses keys and values to store data. You can see the difference below. \n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "## Lists are contained in brackets\n",
    "a = [1, 2, 3, 4]\n",
    "\n",
    "## Empty list\n",
    "a = []\n",
    "\n",
    "## Dictionary uses brackets and key and value pairs\n",
    "\n",
    "a = {\n",
    "    'First_Name' : 'Chad',\n",
    "    'Last_Name' : 'Day'\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "We'll be using a list of dictionaries to store the data we scrape as below.\n",
    "\n",
    "Let's start by just gathering the first item in each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here's our empty list where we'll store each dictionary \n",
    "data = []\n",
    "\n",
    "for row in rows: ## Here's our for loop; note the colon\n",
    "    tds = row.find_all('td') ## Creates a list of the data in tds\n",
    "    row_dict = { ## Note we create our dictionary here\n",
    "        'date': tds[0].text\n",
    "    }\n",
    "    data.append(row_dict) ## We use the append method to add to our data list.\n",
    "\n",
    "print(data[0:5])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the filing name, the filer's name and title\n",
    "\n",
    "Note that we're using a process called `indexing` here. Computers count from zero so the first item in our list of `tds` is `0`. In Python, we use brackets next to a list to select just that item. \n",
    "```\n",
    "tds[0] ## First item \n",
    "tds[2] ## Third item\n",
    "tds[3] ## Fourth item\n",
    "```\n",
    "\n",
    "Below we'll gather the filing name, filer's name and title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for row in rows: ## Here's our for loop; note the colon\n",
    "    tds = row.find_all('td')\n",
    "    row_dict = {\n",
    "        'date': tds[0].text,\n",
    "        'filer_name': tds[2].text,\n",
    "        'agency': tds[3].text,\n",
    "        'title': tds[4].text\n",
    "    }\n",
    "    data.append(row_dict)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting urls \n",
    "\n",
    "The only thing missing now is the url of the filing. In HTML, it is stored using the `a` tag and specified as `href`. Return to the Inspector on the page or see below.\n",
    "\n",
    "```\n",
    "<td nowrap><font size=\"2\" ''face=\"Verdana\">\n",
    "        <a \n",
    "            href= 'https://extapps2.oge.gov/201/Presiden.nsf/PAS+Index/CA12044E3E1DB208852587A40033D508/$FILE/Young, \"Shalanda  final278.pdf'> Nominee 278 (12/03/2021)\n",
    "        </a>\n",
    "</font></td>\n",
    "\n",
    "```\n",
    "\n",
    "We'll use BeautifulSoup here to grab it from the second item in our `tds` list for each row, which as you remember is accessed using `[1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for row in rows: ## Here's our for loop; note the colon\n",
    "    tds = row.find_all('td')\n",
    "    url = tds[1].find('a').get('href')\n",
    "    file_name = tds[1].find('a').text\n",
    "    row_dict = {\n",
    "        'date': tds[0].text,\n",
    "        'url': url,\n",
    "        'file_name': file_name,\n",
    "        'filing': tds[2].text,\n",
    "        'filers_name': tds[3].text,\n",
    "        'title': tds[4].text\n",
    "    }\n",
    "    data.append(row_dict)\n",
    "\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too bad but we're missing one last thing. The domain of the url is missing. So let's use a process called string concatenation to add it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for row in rows: ## Here's our for loop; note the colon\n",
    "    tds = row.find_all('td')\n",
    "    url_start = 'https://extapps2.oge.gov'\n",
    "    url_end = tds[1].find('a').get('href')\n",
    "    url = url_start + url_end\n",
    "    file_name = tds[1].find('a').text\n",
    "    row_dict = {\n",
    "        'date': tds[0].text,\n",
    "        'url': url,\n",
    "        'file_name': file_name,\n",
    "        'filer_name': tds[2].text,\n",
    "        'agency': tds[3].text,\n",
    "        'title': tds[4].text\n",
    "    }\n",
    "    data.append(row_dict)\n",
    "\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data in a list of dictionaries, let's save it to a file. Python has a built in module called `csv` that writes data to a CSV file but I prefer using the data science libary `pandas`, which we imported earlier in the notebook.\n",
    "\n",
    "`pandas` uses spreadsheet like objects called dataframes that have columns and rows. Now that we have a list of dictionaries containing our data, we can save it as a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And save it as a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! You have scraped data from a website and saved it to a CSV!\n",
    "\n",
    "Hopefully, this helped you feel more comfortable working with data in Python. You can also run this as a script as laid out below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import pprint\n",
    "\n",
    "def main():\n",
    "    url = 'https://extapps2.oge.gov/201/Presiden.nsf/PAS%20Filings%20by%20Date?OpenView'\n",
    "    header = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36 (my-email-here)'}\n",
    "    r = requests.get(url, headers=header)\n",
    "    html = r.text\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    table = soup.find_all('table')\n",
    "    rows = table[0].find_all('tr')\n",
    "    data = []\n",
    "    for row in rows: \n",
    "        parse_rows(row, data)\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv('./data.csv')\n",
    "\n",
    "def parse_rows(row, data):\n",
    "    tds = row.find_all('td')\n",
    "    url_start = 'https://extapps2.oge.gov'\n",
    "    url_end = tds[1].find('a').get('href')\n",
    "    url = url_start + url_end\n",
    "    row_dict = {\n",
    "        'date': tds[0].text,\n",
    "        'url': url,\n",
    "        'filing': tds[2].text,\n",
    "        'filers_name': tds[3].text,\n",
    "        'title': tds[4].text\n",
    "    }\n",
    "    data.append(row_dict)\n",
    "\n",
    "\n",
    "main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
